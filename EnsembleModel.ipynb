{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function, division\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "root_dir = \"Dataset/\"\n",
    "epochs = 1\n",
    "batch_size = 3\n",
    "maxFaces = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9808, 3341, 1000]\n"
     ]
    }
   ],
   "source": [
    "neg_train = sorted(os.listdir('Dataset/emotiw/train/'+'Negative/'))\n",
    "neu_train = sorted(os.listdir('Dataset/emotiw/train/'+'Neutral/'))\n",
    "pos_train = sorted(os.listdir('Dataset/emotiw/train/'+'Positive/'))\n",
    "\n",
    "train_filelist = neg_train + neu_train + pos_train\n",
    "\n",
    "val_filelist = []\n",
    "test_filelist = []\n",
    "\n",
    "with open('Dataset/val_list', 'rb') as fp:\n",
    "    val_filelist = pickle.load(fp)\n",
    "\n",
    "with open('Dataset/test_list', 'rb') as fp:\n",
    "    test_filelist = pickle.load(fp)\n",
    "\n",
    "for i in train_filelist:\n",
    "    if i[0] != 'p' and i[0] != 'n':\n",
    "        train_filelist.remove(i)\n",
    "        \n",
    "for i in val_filelist:\n",
    "    if i[0] != 'p' and i[0] != 'n':\n",
    "        val_filelist.remove(i)\n",
    "\n",
    "dataset_sizes = [len(train_filelist), len(val_filelist), len(test_filelist)]\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_global_data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_faces_data_transform = transforms.Compose([\n",
    "        transforms.Resize((96,112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "val_faces_data_transform = transforms.Compose([\n",
    "        transforms.Resize((96,112)),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotiWDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filelist, root_dir, loadTrain=True, transformGlobal=transforms.ToTensor(), transformFaces = transforms.ToTensor()):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filelist: List of names of image/feature files.\n",
    "            root_dir: Dataset directory\n",
    "            transform (callable, optional): Optional transformer to be applied\n",
    "                                            on an image sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filelist = filelist\n",
    "        self.root_dir = root_dir\n",
    "        self.transformGlobal = transformGlobal\n",
    "        self.transformFaces = transformFaces\n",
    "        self.loadTrain = loadTrain\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.loadTrain:\n",
    "            return (len(train_filelist)) \n",
    "        else:\n",
    "            return (len(val_filelist))  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        train = ''\n",
    "        if self.loadTrain:\n",
    "            train = 'train'\n",
    "        else:\n",
    "            train = 'val'\n",
    "        filename = self.filelist[idx].split('.')[0]\n",
    "        labeldict = {'neg':'Negative',\n",
    "                     'neu':'Neutral',\n",
    "                     'pos':'Positive',\n",
    "                     'Negative': 0,\n",
    "                     'Neutral': 1,\n",
    "                     'Positive':2}\n",
    "\n",
    "        labelname = labeldict[filename.split('_')[0]]\n",
    "\n",
    "        #IMAGE \n",
    "\n",
    "        image = Image.open(self.root_dir+'emotiw/'+train+'/'+labelname+'/'+filename+'.jpg')\n",
    "        if self.transformGlobal:\n",
    "            image = self.transformGlobal(image)\n",
    "        if image.shape[0] == 1:\n",
    "            image_1 = np.zeros((3, 224, 224), dtype = float)\n",
    "            image_1[0] = image\n",
    "            image_1[1] = image\n",
    "            image_1[2] = image\n",
    "            image = image_1\n",
    "            image = torch.FloatTensor(image.tolist()) \n",
    "        \n",
    "        #FEATURES FROM MTCNN\n",
    "\n",
    "        features = np.load(self.root_dir+'FaceFeatures/'+train+'/'+labelname+'/'+filename+'.npz')['a']\n",
    "        numberFaces = features.shape[0]\n",
    "        maxNumber = min(numberFaces, maxFaces)\n",
    "        \n",
    "\n",
    "        features1 = np.zeros((maxFaces, 256), dtype = 'float32')\n",
    "        for i in range(maxNumber):\n",
    "            features1[i] = features[i]\n",
    "        features1 = torch.from_numpy(features1)\n",
    "\n",
    "        #ALIGNED CROPPED FACE IMAGES\n",
    "\n",
    "        features2 = np.zeros((maxFaces, 3, 96, 112), dtype = 'float32')\n",
    "#         print(maxNumber)\n",
    "        \n",
    "        for i in range(maxNumber):\n",
    "            face = Image.open(self.root_dir + 'AlignedCroppedImages/'+train+'/'+ labelname + '/' + filename+ '_' + str(i) + '.jpg')\n",
    "                \n",
    "            if self.transformFaces:\n",
    "                face = self.transformFaces(face)\n",
    "                \n",
    "            features2[i] = face.numpy()\n",
    "            \n",
    "        features2 = torch.from_numpy(features2)\n",
    "\n",
    "        #SAMPLE\n",
    "        sample = {'image': image, 'features_mtcnn': features1, 'features_aligned':features2, 'label':labeldict[labelname], 'numberFaces': numberFaces}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EmotiWDataset(train_filelist, root_dir, loadTrain = True, transformGlobal=train_global_data_transform, transformFaces=train_faces_data_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "val_dataset = EmotiWDataset(val_filelist, root_dir, loadTrain=False, transformGlobal = val_global_data_transform, transformFaces=val_faces_data_transform)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, shuffle =True, batch_size = batch_size, num_workers = 0)\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SphereFace Model For Aligned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSoftmaxLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, margin):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.margin = margin\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "\n",
    "        self.divisor = math.pi / self.margin\n",
    "        self.coeffs = binom(margin, range(0, margin + 1, 2))\n",
    "        self.cos_exps = range(self.margin, -1, -2)\n",
    "        self.sin_sq_exps = range(len(self.cos_exps))\n",
    "        self.signs = [1]\n",
    "        for i in range(1, len(self.sin_sq_exps)):\n",
    "            self.signs.append(self.signs[-1] * -1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_normal(self.weight.data.t())\n",
    "\n",
    "    def find_k(self, cos):\n",
    "        acos = cos.acos()\n",
    "        k = (acos / self.divisor).floor().detach()\n",
    "        return k\n",
    "\n",
    "    def forward(self, input, target=None):\n",
    "        if self.training:\n",
    "            assert target is not None\n",
    "            logit = input.matmul(self.weight)\n",
    "            batch_size = logit.size(0)\n",
    "            logit_target = logit[range(batch_size), target]\n",
    "            weight_target_norm = self.weight[:, target].norm(p=2, dim=0)\n",
    "            input_norm = input.norm(p=2, dim=1)\n",
    "            norm_target_prod = weight_target_norm * input_norm\n",
    "            cos_target = logit_target / (norm_target_prod + 1e-10)\n",
    "            sin_sq_target = 1 - cos_target**2\n",
    "            \n",
    "            weight_nontarget_norm = self.weight.norm(p=2, dim=0)\n",
    "            \n",
    "            norm_nontarget_prod = torch.zeros((batch_size,numClasses), dtype = torch.float)\n",
    "            logit2 = torch.zeros((batch_size,numClasses), dtype = torch.float)\n",
    "            logit3 = torch.zeros((batch_size,numClasses), dtype = torch.float)\n",
    "\n",
    "            norm_nontarget_prod = norm_nontarget_prod.to(device)\n",
    "            logit2 = logit2.to(device)\n",
    "            logit3 = logit3.to(device)\n",
    "            \n",
    "            for i in range(numClasses):\n",
    "                norm_nontarget_prod[:, i] = weight_nontarget_norm[i] * input_norm \n",
    "                logit2[:, i] = norm_target_prod / (norm_nontarget_prod[:, i] + 1e-10)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                for j in range(numClasses):\n",
    "                    logit3[i][j] = logit2[i][j] * logit[i][j]\n",
    "\n",
    "            num_ns = self.margin//2 + 1\n",
    "            coeffs = Variable(input.data.new(self.coeffs))\n",
    "            cos_exps = Variable(input.data.new(self.cos_exps))\n",
    "            sin_sq_exps = Variable(input.data.new(self.sin_sq_exps))\n",
    "            signs = Variable(input.data.new(self.signs))\n",
    "\n",
    "            cos_terms = cos_target.unsqueeze(1) ** cos_exps.unsqueeze(0)\n",
    "            sin_sq_terms = (sin_sq_target.unsqueeze(1)\n",
    "                            ** sin_sq_exps.unsqueeze(0))\n",
    "\n",
    "            cosm_terms = (signs.unsqueeze(0) * coeffs.unsqueeze(0)\n",
    "                          * cos_terms * sin_sq_terms)\n",
    "            cosm = cosm_terms.sum(1)\n",
    "            k = self.find_k(cos_target)\n",
    "            \n",
    "            ls_target = norm_target_prod * (((-1)**k * cosm) - 2*k)\n",
    "            logit3[range(batch_size), target] = ls_target\n",
    "            return logit\n",
    "        else:\n",
    "            assert target is None\n",
    "            return input.matmul(self.weight)\n",
    "\n",
    "class sphere20a(nn.Module):\n",
    "    def __init__(self,classnum=3,feature=False):\n",
    "        super(sphere20a, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.feature = feature\n",
    "        #input = B*3*112*96\n",
    "        self.conv1_1 = nn.Conv2d(3,64,3,2,1) #=>B*64*56*48\n",
    "        self.relu1_1 = nn.PReLU(64)\n",
    "        self.conv1_2 = nn.Conv2d(64,64,3,1,1)\n",
    "        self.relu1_2 = nn.PReLU(64)\n",
    "        self.conv1_3 = nn.Conv2d(64,64,3,1,1)\n",
    "        self.relu1_3 = nn.PReLU(64)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64,128,3,2,1) #=>B*128*28*24\n",
    "        self.relu2_1 = nn.PReLU(128)\n",
    "        self.conv2_2 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.relu2_2 = nn.PReLU(128)\n",
    "        self.conv2_3 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.relu2_3 = nn.PReLU(128)\n",
    "\n",
    "        self.conv2_4 = nn.Conv2d(128,128,3,1,1) #=>B*128*28*24\n",
    "        self.relu2_4 = nn.PReLU(128)\n",
    "        self.conv2_5 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.relu2_5 = nn.PReLU(128)\n",
    "\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128,256,3,2,1) #=>B*256*14*12\n",
    "        self.relu3_1 = nn.PReLU(256)\n",
    "        self.conv3_2 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.relu3_2 = nn.PReLU(256)\n",
    "        self.conv3_3 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.relu3_3 = nn.PReLU(256)\n",
    "\n",
    "        self.conv3_4 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n",
    "        self.relu3_4 = nn.PReLU(256)\n",
    "        self.conv3_5 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.relu3_5 = nn.PReLU(256)\n",
    "\n",
    "        self.conv3_6 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n",
    "        self.relu3_6 = nn.PReLU(256)\n",
    "        self.conv3_7 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.relu3_7 = nn.PReLU(256)\n",
    "\n",
    "        self.conv3_8 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n",
    "        self.relu3_8 = nn.PReLU(256)\n",
    "        self.conv3_9 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.relu3_9 = nn.PReLU(256)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256,512,3,2,1) #=>B*512*7*6\n",
    "        self.relu4_1 = nn.PReLU(512)\n",
    "        self.conv4_2 = nn.Conv2d(512,512,3,1,1)\n",
    "        self.relu4_2 = nn.PReLU(512)\n",
    "        self.conv4_3 = nn.Conv2d(512,512,3,1,1)\n",
    "        self.relu4_3 = nn.PReLU(512)\n",
    "\n",
    "        self.fc5 = nn.Linear(512*7*6,512)\n",
    "        self.fc6 = LSoftmaxLinear(512,self.classnum, 4)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.relu1_1(self.conv1_1(x))\n",
    "        x = x + self.relu1_3(self.conv1_3(self.relu1_2(self.conv1_2(x))))\n",
    "\n",
    "        x = self.relu2_1(self.conv2_1(x))\n",
    "        x = x + self.relu2_3(self.conv2_3(self.relu2_2(self.conv2_2(x))))\n",
    "        x = x + self.relu2_5(self.conv2_5(self.relu2_4(self.conv2_4(x))))\n",
    "\n",
    "        x = self.relu3_1(self.conv3_1(x))\n",
    "        x = x + self.relu3_3(self.conv3_3(self.relu3_2(self.conv3_2(x))))\n",
    "        x = x + self.relu3_5(self.conv3_5(self.relu3_4(self.conv3_4(x))))\n",
    "        x = x + self.relu3_7(self.conv3_7(self.relu3_6(self.conv3_6(x))))\n",
    "        x = x + self.relu3_9(self.conv3_9(self.relu3_8(self.conv3_8(x))))\n",
    "\n",
    "        x = self.relu4_1(self.conv4_1(x))\n",
    "        x = x + self.relu4_3(self.conv4_3(self.relu4_2(self.conv4_2(x))))\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = (self.fc5(x))\n",
    "#         print(x)\n",
    "        if self.feature: return x\n",
    "\n",
    "        x = self.fc6(x)\n",
    "#         x = self.fc6(x, None)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 \n",
    "## Pretrained EmotiW DenseNet (DenseNet161_EmotiW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained EmotiW DenseNet Loaded! (Model 1)\n"
     ]
    }
   ],
   "source": [
    "global_model = torch.load('./TrainedModels/EnsembleModels/DenseNet161_EmotiW', map_location=lambda storage, loc: storage).module\n",
    "model1 = global_model\n",
    "print('Pretrained EmotiW DenseNet Loaded! (Model 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "## Pretrained EmotiC DenseNet (densenet_emotiw_pretrainemotic_lr001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained EmotiC DenseNet Loaded! (Model 2)\n"
     ]
    }
   ],
   "source": [
    "model2 = models.densenet161(pretrained=False)\n",
    "num_ftrs = model2.classifier.in_features\n",
    "model2.classifier = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model2 = model2.to(device)\n",
    "model2 = nn.DataParallel(model2)\n",
    "model2.load_state_dict(torch.load('./TrainedModels/EnsembleModels/densenet_emotiw_pretrainemotic_lr001.pt', map_location=lambda storage, loc: storage))\n",
    "model2 = model2.module\n",
    "\n",
    "print('Pretrained EmotiC DenseNet Loaded! (Model 2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "## Aligned Model Global Level (AlignedModelTrainerSoftmax_AlignedModel_EmotiW_lr01_Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, non_align_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.non_align_model = non_align_model\n",
    "    \n",
    "    def forward(self, face_features_initial, numberFaces, labels):\n",
    "        \n",
    "        maxNumber = np.minimum(numberFaces, maxFaces).float()\n",
    "        maxNumber = maxNumber.to(device)\n",
    "\n",
    "        face_features = torch.zeros((face_features_initial.shape[0],maxFaces,3), dtype = torch.float)\n",
    "        \n",
    "        for j in range(face_features_initial.shape[0]):\n",
    "            face = face_features_initial[j]\n",
    "            tensor = torch.zeros((2,), dtype=torch.long)\n",
    "            faceLabels = tensor.new_full((maxFaces,), labels[j], dtype = torch.long)\n",
    "            faceLabels = faceLabels.to(device)\n",
    "            face_features[j, :, :] = self.non_align_model.forward(face, faceLabels)\n",
    "            \n",
    "        face_features = face_features.to(device)\n",
    "        \n",
    "        face_features_sum = torch.zeros((face_features_initial.shape[0], 3), dtype = torch.float)\n",
    "        face_features_sum = face_features_sum.to(device)\n",
    "        \n",
    "        face_features_avg = torch.zeros((face_features_initial.shape[0], 3), dtype = torch.float)\n",
    "        face_features_avg = face_features_avg.to(device)\n",
    "\n",
    "        for i in range(face_features_initial.shape[0]):\n",
    "            for j in range(int(maxNumber[i])):\n",
    "                face_features_sum[i] = face_features_sum[i] + face_features[i][j]\n",
    "                \n",
    "            if int(maxNumber[i]) != 0:\n",
    "                y = float(maxNumber[i])\n",
    "                face_features_avg[i] = face_features_sum[i] / y\n",
    "\n",
    "        return face_features_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Model Global Level Loaded! (Model 3)\n"
     ]
    }
   ],
   "source": [
    "aligned_model_global_level_path = \"./TrainedModels/EnsembleModels/AlignedModelTrainerSoftmax_AlignedModel_EmotiW_lr01_Softmax\"\n",
    "align_model = torch.load(aligned_model_global_level_path, map_location=lambda storage, loc: storage).module\n",
    "model3 = align_model\n",
    "print('Aligned Model Global Level Loaded! (Model 3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 \n",
    "## Aligned Model Image Level Trained (AlignedModel_EmotiW_lr01_Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Model Image Level Loaded! (Model 4)\n"
     ]
    }
   ],
   "source": [
    "aligned_model_image_level_path = './TrainedModels/EnsembleModels/AlignedModel_EmotiW_lr01_Softmax'\n",
    "align_model = torch.load(aligned_model_image_level_path, map_location=lambda storage, loc: storage).module\n",
    "\n",
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, non_align_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.non_align_model = non_align_model\n",
    "    \n",
    "    def forward(self, face_features_initial, numberFaces, labels):\n",
    "#         global_features_initial = self.global_model.forward(image)\n",
    "# #         print(global_features_initial)\n",
    "#         global_features_initial = Variable(global_features_initial)\n",
    "#         global_features = global_features_initial.view(-1,1,256)\n",
    "#         batch_size = global_features.shape[0]\n",
    "        \n",
    "        maxNumber = np.minimum(numberFaces, maxFaces)\n",
    "        maxNumber = maxNumber.to(device)\n",
    "\n",
    "        face_features = torch.zeros((face_features_initial.shape[0],maxFaces,3), dtype = torch.float)\n",
    "        \n",
    "        for j in range(face_features_initial.shape[0]):\n",
    "#             for i in range(int(maxNumber[j])):\n",
    "            face = face_features_initial[j]\n",
    "            # print(face.shape)\n",
    "#                 face = face.squeeze(1)\n",
    "            # print(face.shape)\n",
    "            face_features[j, :, :] = self.non_align_model.forward(face, labels)\n",
    "            \n",
    "        face_features = face_features.to(device)\n",
    "        face_features_sum = torch.zeros((face_features_initial.shape[0], 3), dtype = torch.float)\n",
    "\n",
    "        face_features_sum = face_features_sum.to(device)\n",
    "\n",
    "        for i in range(face_features_initial.shape[0]):\n",
    "            for j in range(int(maxNumber[i])):\n",
    "                face_features_sum[i] = face_features_sum[i] + face_features[i][j]\n",
    "                \n",
    "            face_features_sum[i] = face_features_sum[i] / (maxNumber[i].float())\n",
    "            \n",
    "        return face_features_sum\n",
    "\n",
    "\n",
    "model4 = FaceAttention(align_model)\n",
    "print('Aligned Model Image Level Loaded! (Model 4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "## Avg. Face Features Concat Model (PretrainedDenseNetAvgFaceFeatures-FineTune-2208-3-NoSoftmax-Reg-lr001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Face Features Concat Model Loaded! (Model 5)\n"
     ]
    }
   ],
   "source": [
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, global_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.global_fc3_debug = nn.Linear(2464, 3)\n",
    "        nn.init.kaiming_normal_(self.global_fc3_debug.weight)\n",
    "        self.global_fc3_debug.bias.data.fill_(0.01)\n",
    "        self.bn_global = nn.BatchNorm1d(2208, affine=False)\n",
    "        self.bn_face_features = nn.BatchNorm1d(256, affine=False)\n",
    "        self.dropout_classifier = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, image, face_features, numberFaces):\n",
    "        features = self.global_model.forward(image)\n",
    "\n",
    "        out = F.relu(features, inplace=True)\n",
    "        global_features_initial = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "\n",
    "        global_features_initial = Variable(global_features_initial)\n",
    "        batch_size = face_features.shape[0]\n",
    "        global_features_initial = global_features_initial.view(-1,2208)\n",
    "        face_features_sum = torch.sum(face_features, dim=1)\n",
    "        face_features_sum = face_features_sum.view(-1, 256)\n",
    "        for i in range(batch_size):\n",
    "            faces_num_div = float(min(numberFaces[i], maxFaces))\n",
    "            if faces_num_div != 0:\n",
    "                face_features_sum[i] = face_features_sum[i] / faces_num_div\n",
    "        #THE face_features_sum TENSOR NOW CONTAINS AVERAGE OF THE FACE FEATURES\n",
    "\n",
    "        face_features_sum = self.bn_face_features(face_features_sum)\n",
    "        global_features_initial = self.bn_global(global_features_initial)\n",
    "\n",
    "        final_features = torch.cat((face_features_sum, global_features_initial), dim=1)\n",
    "        final_features = self.dropout_classifier(final_features)\n",
    "\n",
    "        x = (self.global_fc3_debug(final_features))\n",
    "        return x\n",
    "    \n",
    "model5 = torch.load('./TrainedModels/EnsembleModels/PretrainedDenseNet-FineTune-2208-3-lr001-Regularized-Corrected', map_location=lambda storage, loc: storage).module\n",
    "print('Avg. Face Features Concat Model Loaded! (Model 5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6\n",
    "## Face Attention Model (EmotiC) using 3rd Para Attention (FaceAttention_AlignedModel_FullTrain_3para_lr001_dropout_BN_SoftmaxLr01_EmotiC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Attention Model (EmotiC) using 3rd Para Attention Loaded! (Model 6)\n"
     ]
    }
   ],
   "source": [
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, global_model, non_align_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.non_align_model = non_align_model\n",
    "        \n",
    "        self.global_fc3_debug = nn.Linear(320, 3)\n",
    "        nn.init.kaiming_normal_(self.global_fc3_debug.weight)\n",
    "        self.global_fc3_debug.bias.data.fill_(0.01)\n",
    "\n",
    "        self.global_fc = nn.Linear(256, 64)\n",
    "        nn.init.kaiming_normal_(self.global_fc.weight)\n",
    "        self.global_fc.bias.data.fill_(0.01)   \n",
    "\n",
    "        self.global_fc_dropout = nn.Dropout(p = 0.5)\n",
    "        self.global_fc_main_dropout = nn.Dropout(p = 0.5)\n",
    "        self.non_align_model_dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "        self.bn_debug_face = nn.BatchNorm1d(64, affine=False)\n",
    "        self.bn_debug_global = nn.BatchNorm1d(256, affine=False)\n",
    "    \n",
    "    def forward(self, image, face_features_initial, numberFaces, labels):\n",
    "\n",
    "        features = self.global_model.forward(image)\n",
    "\n",
    "        global_features_main = self.global_fc_main_dropout(features)\n",
    "        \n",
    "        global_features = self.global_fc_dropout(self.global_fc(global_features_main))\n",
    "\n",
    "        global_features = global_features.view(-1,1,64)\n",
    "\n",
    "        batch_size = global_features.shape[0]\n",
    "        \n",
    "        maxNumber = np.minimum(numberFaces, maxFaces)\n",
    "\n",
    "        face_features = torch.zeros((batch_size,maxFaces,64), dtype = torch.float)\n",
    "        \n",
    "        face_features = face_features.to(device)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            face = face_features_initial[j]\n",
    "            face_features[j, :, :] = self.non_align_model.forward(face, labels)\n",
    "        \n",
    "        face_features = self.non_align_model_dropout(face_features)\n",
    "\n",
    "        face_features = face_features.view(batch_size, 64, -1)\n",
    "\n",
    "        mask = np.zeros((batch_size,1,maxFaces), dtype = 'float32')\n",
    "        for j in range(batch_size):\n",
    "            for i in range(maxFaces - (int(maxNumber[j]))):\n",
    "                mask[j][0][int(numberFaces[j]) + i] = float('-inf')\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.to(device)\n",
    "        attention_scores = torch.bmm(global_features, face_features) #(batch_size, 1, 256) x (batch_size, 256, nFaces) = (batch_size, 1, nFaces)\n",
    "        attention_scores = attention_scores+mask\n",
    "                \n",
    "        #Convert Scores to Weight\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1)\n",
    "        \n",
    "        attention_weights = attention_scores\n",
    "#         print(attention_scores.shape)\n",
    "        # print(attention_scores)\n",
    "        \n",
    "        attention_weights = Variable(attention_scores)\n",
    "        \n",
    "        for i in range(len(maxNumber)):\n",
    "            if maxNumber[i] == 0:\n",
    "                for j in range(maxFaces):\n",
    "                    attention_weights[i][0][j] =  0 \n",
    "        \n",
    "        #Taking Weighted Average of Face Featrues\n",
    "        face_features = face_features.view(batch_size, -1, 64) #(batch_size, nFaces, 256)\n",
    "        attention_scores = attention_weights.view(batch_size, 1, -1) #(batch_size, 1, nFaces)\n",
    "        attended_face_features = torch.bmm(attention_scores, face_features)\n",
    "        \n",
    "        #Concatenating Global and Attended Face Features \n",
    "        attended_face_features = attended_face_features.view(batch_size, -1)\n",
    "        # global_features = global_features.view(batch_size, -1)\n",
    "        attended_face_features = self.bn_debug_face(attended_face_features)\n",
    "        global_features_main = self.bn_debug_global(global_features_main)\n",
    "        final_features = torch.cat((attended_face_features, global_features_main), dim=1)\n",
    "        \n",
    "        x = (self.global_fc3_debug(final_features))        \n",
    "        return x\n",
    "\n",
    "model6 = torch.load('./TrainedModels/EnsembleModels/FaceAttention_AlignedModel_FullTrain_3para_lr001_dropout_BN_SoftmaxLr01_EmotiC', map_location=lambda storage, loc: storage).module\n",
    "print('Face Attention Model (EmotiC) using 3rd Para Attention Loaded! (Model 6)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7\n",
    "## Face Attention Model (EmotiC) using 4th Para Attention (FaceAttention_AlignedModel_FullTrain_4para_lr001_dropout_BN_SoftmaxLr01_EmotiC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Attention Model (EmotiC) using 4rd Para Attention Loaded! (Model 7)\n"
     ]
    }
   ],
   "source": [
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, global_model, non_align_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.non_align_model = non_align_model\n",
    "        \n",
    "        self.global_fc3_debug = nn.Linear(512, 3)\n",
    "        nn.init.kaiming_normal_(self.global_fc3_debug.weight)\n",
    "        self.global_fc3_debug.bias.data.fill_(0.01)\n",
    "\n",
    "        self.attentionfc1 = nn.Linear(256, 64)\n",
    "        nn.init.kaiming_normal_(self.attentionfc1.weight)\n",
    "        self.attentionfc1.bias.data.fill_(0.01)   \n",
    "\n",
    "        self.attentionfc2 = nn.Linear(64, 1)\n",
    "        nn.init.kaiming_normal_(self.attentionfc2.weight)\n",
    "        self.attentionfc2.bias.data.fill_(0.01)\n",
    "\n",
    "        self.attentionfc1_dropout = nn.Dropout(p = 0.5)\n",
    "        self.global_fc_main_dropout = nn.Dropout(p = 0.5)\n",
    "        self.non_align_model_dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "        self.bn_debug_face = nn.BatchNorm1d(256, affine=False)\n",
    "        self.bn_debug_global = nn.BatchNorm1d(256, affine=False)\n",
    "    \n",
    "    def forward(self, image, face_features_initial, numberFaces, labels):\n",
    "\n",
    "        features = self.global_model.forward(image)\n",
    "\n",
    "        global_features = self.global_fc_main_dropout(features)\n",
    "        \n",
    "        batch_size = global_features.shape[0]\n",
    "\n",
    "        global_features = global_features.view(-1,1,256)\n",
    "\n",
    "        \n",
    "        maxNumber = np.minimum(numberFaces, maxFaces)\n",
    "\n",
    "        face_features = torch.zeros((batch_size,maxFaces,256), dtype = torch.float)\n",
    "        \n",
    "        face_features = face_features.to(device)\n",
    "\n",
    "        mid_face_features = torch.zeros((batch_size, maxFaces, 1), dtype = torch.float)\n",
    "        face_features_inter = torch.zeros((batch_size, maxFaces, 64), dtype = torch.float)\n",
    "        face_features_inter = face_features_inter.to(device)\n",
    "        mid_face_features = mid_face_features.to(device)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            face = face_features_initial[j]\n",
    "            face_features[j, :, :] = self.non_align_model_dropout(self.non_align_model.forward(face, labels))\n",
    "            face_features_inter[j] = self.attentionfc1_dropout(self.attentionfc1(face_features[j]))\n",
    "            mid_face_features[j] = self.attentionfc2(face_features_inter[j])\n",
    "        \n",
    "    \n",
    "        mid_face_features = mid_face_features.view(batch_size, 1, maxFaces)\n",
    "\n",
    "        mask = np.zeros((batch_size,1,maxFaces), dtype = 'float32')\n",
    "        for j in range(batch_size):\n",
    "            for i in range(maxFaces - (int(maxNumber[j]))):\n",
    "                mask[j][0][int(numberFaces[j]) + i] = float('-inf')\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.to(device)\n",
    "        attention_scores = mid_face_features + mask\n",
    "        \n",
    "        #Convert Scores to Weight\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1)\n",
    "        \n",
    "        attention_weights = Variable(attention_scores)\n",
    "        \n",
    "        for i in range(len(maxNumber)):\n",
    "            if maxNumber[i] == 0:\n",
    "                for j in range(maxFaces):\n",
    "                    attention_weights[i][0][j] =  0 \n",
    "        \n",
    "        #Taking Weighted Average of Face Featrues\n",
    "        face_features = face_features.view(batch_size, -1, 256) #(batch_size, nFaces, 256)\n",
    "        attention_scores = attention_weights.view(batch_size, 1, -1) #(batch_size, 1, nFaces)\n",
    "        attended_face_features = torch.bmm(attention_scores, face_features)\n",
    "        \n",
    "        #Concatenating Global and Attended Face Features \n",
    "        attended_face_features = attended_face_features.view(batch_size, -1)\n",
    "        global_features = global_features.view(batch_size, -1)\n",
    "        \n",
    "        attended_face_features = self.bn_debug_face(attended_face_features)\n",
    "        global_features = self.bn_debug_global(global_features)\n",
    "\n",
    "        final_features = torch.cat((attended_face_features, global_features), dim=1)\n",
    "        \n",
    "        x = (self.global_fc3_debug(final_features))        \n",
    "        return x\n",
    "    \n",
    "model7 = torch.load('./TrainedModels/EnsembleModels/FaceAttention_AlignedModel_FullTrain_4para_lr001_dropout_BN_SoftmaxLr01_EmotiC', map_location=lambda storage, loc: storage).module\n",
    "print('Face Attention Model (EmotiC) using 4rd Para Attention Loaded! (Model 7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 8\n",
    "## Face Attention Model using 4th Para Attention (FaceAttention_AlignedModel_FullTrain_4para_lr01_dropout_BN_SoftmaxLr01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Attention Model using 4rd Para Attention Loaded! (Model 8)\n"
     ]
    }
   ],
   "source": [
    "class FaceAttention(nn.Module):\n",
    "    def __init__(self, global_model, non_align_model):\n",
    "        super(FaceAttention, self).__init__()\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.non_align_model = non_align_model\n",
    "        \n",
    "        self.global_fc3_debug = nn.Linear(512, 3)\n",
    "        nn.init.kaiming_normal_(self.global_fc3_debug.weight)\n",
    "        self.global_fc3_debug.bias.data.fill_(0.01)\n",
    "\n",
    "        self.attentionfc1 = nn.Linear(256, 64)\n",
    "        nn.init.kaiming_normal_(self.attentionfc1.weight)\n",
    "        self.attentionfc1.bias.data.fill_(0.01)   \n",
    "\n",
    "        self.attentionfc2 = nn.Linear(64, 1)\n",
    "        nn.init.kaiming_normal_(self.attentionfc2.weight)\n",
    "        self.attentionfc2.bias.data.fill_(0.01)\n",
    "\n",
    "        self.global_fc_main = nn.Linear(2208, 256)\n",
    "        nn.init.kaiming_normal_(self.global_fc_main.weight)\n",
    "        self.global_fc_main.bias.data.fill_(0.01)\n",
    "\n",
    "        self.attentionfc1_dropout = nn.Dropout(p = 0.5)\n",
    "        self.global_fc_main_dropout = nn.Dropout(p = 0.5)\n",
    "        self.non_align_model_dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "        self.bn_debug_face = nn.BatchNorm1d(256, affine=False)\n",
    "        self.bn_debug_global = nn.BatchNorm1d(256, affine=False)\n",
    "    \n",
    "    def forward(self, image, face_features_initial, numberFaces, labels):\n",
    "\n",
    "        features = self.global_model.forward(image)\n",
    "\n",
    "        out = F.relu(features, inplace = False)\n",
    "        global_features_initial = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "\n",
    "        global_features_initial = Variable(global_features_initial)\n",
    "\n",
    "        global_features_initial = global_features_initial.view(-1,2208)\n",
    "\n",
    "        global_features = self.global_fc_main_dropout(self.global_fc_main(global_features_initial))\n",
    "        \n",
    "        batch_size = global_features.shape[0]\n",
    "\n",
    "        global_features = global_features.view(-1,1,256)\n",
    "\n",
    "        \n",
    "        maxNumber = np.minimum(numberFaces, maxFaces)\n",
    "\n",
    "        face_features = torch.zeros((batch_size,maxFaces,256), dtype = torch.float)\n",
    "        \n",
    "        face_features = face_features.to(device)\n",
    "\n",
    "        mid_face_features = torch.zeros((batch_size, maxFaces, 1), dtype = torch.float)\n",
    "        face_features_inter = torch.zeros((batch_size, maxFaces, 64), dtype = torch.float)\n",
    "        face_features_inter = face_features_inter.to(device)\n",
    "        mid_face_features = mid_face_features.to(device)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            face = face_features_initial[j]\n",
    "            face_features[j, :, :] = self.non_align_model_dropout(self.non_align_model.forward(face, labels))\n",
    "            face_features_inter[j] = self.attentionfc1_dropout(self.attentionfc1(face_features[j]))\n",
    "            mid_face_features[j] = self.attentionfc2(face_features_inter[j])\n",
    "            \n",
    "        mid_face_features = mid_face_features.view(batch_size, 1, maxFaces)\n",
    "\n",
    "        mask = np.zeros((batch_size,1,maxFaces), dtype = 'float32')\n",
    "        for j in range(batch_size):\n",
    "            for i in range(maxFaces - (int(maxNumber[j]))):\n",
    "                mask[j][0][int(numberFaces[j]) + i] = float('-inf')\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.to(device)\n",
    "        attention_scores = mid_face_features + mask\n",
    "        \n",
    "        #Convert Scores to Weight\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1)\n",
    "        \n",
    "        attention_weights = Variable(attention_scores)\n",
    "        \n",
    "        for i in range(len(maxNumber)):\n",
    "            if maxNumber[i] == 0:\n",
    "                for j in range(maxFaces):\n",
    "                    attention_weights[i][0][j] =  0 \n",
    "        \n",
    "        #Taking Weighted Average of Face Featrues\n",
    "        face_features = face_features.view(batch_size, -1, 256) #(batch_size, nFaces, 256)\n",
    "        attention_scores = attention_weights.view(batch_size, 1, -1) #(batch_size, 1, nFaces)\n",
    "        attended_face_features = torch.bmm(attention_scores, face_features)\n",
    "        \n",
    "        #Concatenating Global and Attended Face Features \n",
    "        attended_face_features = attended_face_features.view(batch_size, -1)\n",
    "        global_features = global_features.view(batch_size, -1)\n",
    "        \n",
    "        attended_face_features = self.bn_debug_face(attended_face_features)\n",
    "        global_features = self.bn_debug_global(global_features)\n",
    "\n",
    "        final_features = torch.cat((attended_face_features, global_features), dim=1)\n",
    "        \n",
    "        x = (self.global_fc3_debug(final_features))        \n",
    "        return x\n",
    "    \n",
    "model8 = torch.load('./TrainedModels/EnsembleModels/FaceAttention_AlignedModel_FullTrain_4para_lr01_dropout_BN_SoftmaxLr01', map_location=lambda storage, loc: storage).module\n",
    "print('Face Attention Model using 4rd Para Attention Loaded! (Model 8)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8):\n",
    "        super(Ensemble, self).__init__()\n",
    "        \n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.model_3 = model_3\n",
    "        self.model_4 = model_4\n",
    "        self.model_5 = model_5\n",
    "        self.model_6 = model_6\n",
    "        self.model_7 = model_7\n",
    "        self.model_8 = model_8\n",
    "\n",
    "    def forward(self, image, labels, face_features_mtcnn, face_features_aligned, numberFaces):\n",
    "        \n",
    "        output1 = self.model_1(image)\n",
    "        output2 = self.model_2(image)\n",
    "        output3 = self.model_3(face_features_aligned, numberFaces, labels)\n",
    "        output4 = self.model_4(face_features_aligned, numberFaces, labels)\n",
    "        output5 = self.model_5(image, face_features_mtcnn, numberFaces)\n",
    "        output6 = self.model_6(image, face_features_aligned, numberFaces, labels)\n",
    "        output7 = self.model_7(image, face_features_aligned, numberFaces, labels)\n",
    "        output8 = self.model_8(image, face_features_aligned, numberFaces, labels)\n",
    "        \n",
    "        output = 0 * output1 + 1.5 * output2 + 1 * output3 + 0.5 * output4 + 8 * output5 +  1.5 * output6 + 1 * output7 + 1 * output8\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Loaded.\n"
     ]
    }
   ],
   "source": [
    "model_ft = Ensemble(model1, model2, model3, model4, model5, model6, model7, model8)\n",
    "model_ft = model_ft.to(device)\n",
    "model_ft = nn.DataParallel(model_ft)\n",
    "print(\"Ensemble Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer=None, scheduler=None, num_epochs = 1):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in range(1, 2):            \n",
    "            if phase == 0:\n",
    "                dataloaders = train_dataloader\n",
    "                # scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                dataloaders = val_dataloader\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for i_batch, sample_batched in enumerate(dataloaders):\n",
    "                inputs = sample_batched['image']\n",
    "                labels = sample_batched['label']\n",
    "                face_features_mtcnn = sample_batched['features_mtcnn']\n",
    "                face_features_aligned = sample_batched['features_aligned']\n",
    "                numberFaces = sample_batched['numberFaces']\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                face_features_mtcnn= face_features_mtcnn.to(device)\n",
    "                face_features_aligned = face_features_aligned.to(device)\n",
    "                numberFaces = numberFaces.to(device)\n",
    "                \n",
    "                # optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 0):\n",
    "#                     print(\"forward\")\n",
    "                    outputs = model(inputs, labels, face_features_mtcnn, face_features_aligned, numberFaces)\n",
    "#                     print(\"forward \" + str(i_batch))\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #print(\"loss done\")\n",
    "                    \n",
    "                    if phase == 0:\n",
    "                        loss.backward()\n",
    "                        # optimizer.step()\n",
    "                        #print(\"backward done\")\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 1 and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {: .0f}m {:0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "forward\n",
      "torch.Size([3, 3])\n",
      "forward\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8edfedf5b710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-c143f660f71e>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"forward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_features_mtcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_features_aligned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberFaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;31m#                     print(\"forward \" + str(i_batch))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-2bfd2cc958ae>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image, labels, face_features_mtcnn, face_features_aligned, numberFaces)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moutput1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moutput3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_features_aligned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberFaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moutput4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_features_aligned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberFaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\densenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_DenseLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model_ft, criterion, None, None, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_ft.state_dict(), \"FaceAttention_FullTrain_64_1layer_sgd_lr01.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
