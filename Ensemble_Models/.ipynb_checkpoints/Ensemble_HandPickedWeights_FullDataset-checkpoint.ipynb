{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Functions & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name = { 0 : 'Negative',\n",
    "                  1 : 'Neutral',\n",
    "                  2 : 'Positive'}\n",
    "\n",
    "path_eval_output_data = '../ModelOutputs/fourteen_fulltrained_models_output_data.npz'\n",
    "path_test_output_data = '../ModelOutputs/test_data_fourteen_fulltrained_models_outputs.npz'\n",
    "submission_folder = '../Submissions/Submission_8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \n",
    "    Rows are scores for each class. \n",
    "    Columns are predictions (samples).\n",
    "    \"\"\"\n",
    "    scoreMatExp = np.exp(np.asarray(x))\n",
    "    \n",
    "    result = np.zeros((scoreMatExp.shape[0], 3))\n",
    "    \n",
    "    result[:, 0] = scoreMatExp[:, 0] / scoreMatExp.sum(1)\n",
    "    result[:, 1] = scoreMatExp[:, 1] / scoreMatExp.sum(1)\n",
    "    result[:, 2] = scoreMatExp[:, 2] / scoreMatExp.sum(1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "data_eval = np.load(path_eval_output_data)\n",
    "model1_eval = data_eval['output_test_model1']\n",
    "model2_eval = data_eval['output_test_model2']\n",
    "model3_eval = data_eval['output_test_model3']\n",
    "model4_eval = data_eval['output_test_model4']\n",
    "model5_eval = data_eval['output_test_model5']\n",
    "model6_eval = data_eval['output_test_model6']\n",
    "model7_eval = data_eval['output_test_model7']\n",
    "model8_eval = data_eval['output_test_model8']\n",
    "model9_eval = data_eval['output_test_model9']\n",
    "model10_eval = data_eval['output_test_model10']\n",
    "model11_eval = data_eval['output_test_model11']\n",
    "model12_eval = data_eval['output_test_model12']\n",
    "model13_eval = data_eval['output_test_model13']\n",
    "model14_eval = data_eval['output_test_model14']\n",
    "eval_label = data_eval['label_test']\n",
    "print('EVAL Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "data_test = np.load(path_test_output_data)\n",
    "model1_test = data_test['output_test_model1']\n",
    "model2_test = data_test['output_test_model2']\n",
    "model3_test = data_test['output_test_model3']\n",
    "model4_test = data_test['output_test_model4']\n",
    "model5_test = data_test['output_test_model5']\n",
    "model6_test = data_test['output_test_model6']\n",
    "model7_test = data_test['output_test_model7']\n",
    "model8_test = data_test['output_test_model8']\n",
    "model9_test = data_test['output_test_model9']\n",
    "model10_test = data_test['output_test_model10']\n",
    "model11_test = data_test['output_test_model11']\n",
    "model12_test = data_test['output_test_model12']\n",
    "model13_test = data_test['output_test_model13']\n",
    "model14_test = data_test['output_test_model14']\n",
    "test_filenames = data_test['filename_test']\n",
    "print('TEST Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EVAL Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_eval = softmax(model1_eval)\n",
    "model2_eval = softmax(model2_eval)\n",
    "model3_eval = softmax(model3_eval)\n",
    "model4_eval = softmax(model4_eval)\n",
    "model5_eval = softmax(model5_eval)\n",
    "model6_eval = softmax(model6_eval)\n",
    "model7_eval = softmax(model7_eval)\n",
    "model8_eval = softmax(model8_eval)\n",
    "model9_eval = softmax(model9_eval)\n",
    "model10_eval = softmax(model10_eval)\n",
    "model11_eval = softmax(model11_eval)\n",
    "model12_eval = softmax(model12_eval)\n",
    "model13_eval = softmax(model13_eval)\n",
    "model14_eval = softmax(model14_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = 1\n",
    "m2 = 1\n",
    "m3 = 1\n",
    "m4 = 1\n",
    "m5 = 1\n",
    "m6 = 1\n",
    "m7 = 1\n",
    "m8 = 1\n",
    "m9 = 1\n",
    "m10 = 1\n",
    "m11 = 1\n",
    "m12 = 1\n",
    "m13 = 1\n",
    "m14 = 1\n",
    "\n",
    "output_eval = (m1 * model1_eval) + (m2 * model2_eval) + (m3 * model3_eval) + (m4 * model4_eval) +\\\n",
    "(m5*model5_eval) + (m6*model6_eval) + (m7*model7_eval) + (m8*model8_eval) + (m9*model9_eval) +\\\n",
    "(m10*model10_eval) + (m11*model11_eval) + (m12*model12_eval) + (m13*model13_eval) + (m14*model14_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_eval = np.argmax(output_eval, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_eval = np.sum(pred_eval == eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = correct_eval / output_eval.shape[0]\n",
    "print(eval_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test = softmax(model1_test)\n",
    "model2_test = softmax(model2_test)\n",
    "model3_test = softmax(model3_test)\n",
    "model4_test = softmax(model4_test)\n",
    "model5_test = softmax(model5_test)\n",
    "model6_test = softmax(model6_test)\n",
    "model7_test = softmax(model7_test)\n",
    "model8_test = softmax(model8_test)\n",
    "model9_test = softmax(model9_test)\n",
    "model10_test = softmax(model10_test)\n",
    "model11_test = softmax(model11_test)\n",
    "model12_test = softmax(model12_test)\n",
    "model13_test = softmax(model13_test)\n",
    "model14_test = softmax(model14_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = 1\n",
    "m2 = 1\n",
    "m3 = 1\n",
    "m4 = 1\n",
    "m5 = 1\n",
    "m6 = 1\n",
    "m7 = 1\n",
    "m8 = 1\n",
    "m9 = 1\n",
    "m10 = 1\n",
    "m11 = 1\n",
    "m12 = 1\n",
    "m13 = 1\n",
    "m14 = 1\n",
    "\n",
    "output_test = (m1 * model1_test) + (m2 * model2_test) + (m3 * model3_test) + (m4 * model4_test) +\\\n",
    "(m5*model5_test) + (m6*model6_test) + (m7*model7_test) + (m8*model8_test) + (m9*model9_test) +\\\n",
    "(m10*model10_test) + (m11*model11_test) + (m12*model12_test) + (m13*model13_test) + (m14*model14_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = np.argmax(output_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_filenames)):\n",
    "    with open(submission_folder + test_filenames[i] + '.txt', 'w+') as f:\n",
    "        f.write(label_to_name[predictions_test[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
