{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Functions & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name = { 0 : 'Negative',\n",
    "                  1 : 'Neutral',\n",
    "                  2 : 'Positive'}\n",
    "\n",
    "path_output_data = '../ModelOutputs/fourteen_models_output_data.npz'\n",
    "path_test_output_data = '../ModelOutputs/test_data_fourteen_models_outputs.npz'\n",
    "submission_folder = '../Submissions/Submission_8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \n",
    "    Rows are scores for each class. \n",
    "    Columns are predictions (samples).\n",
    "    \"\"\"\n",
    "    scoreMatExp = np.exp(np.asarray(x))\n",
    "    \n",
    "    result = np.zeros((scoreMatExp.shape[0], 3))\n",
    "    \n",
    "    result[:, 0] = scoreMatExp[:, 0] / scoreMatExp.sum(1)\n",
    "    result[:, 1] = scoreMatExp[:, 1] / scoreMatExp.sum(1)\n",
    "    result[:, 2] = scoreMatExp[:, 2] / scoreMatExp.sum(1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "data = np.load(path_output_data)\n",
    "model1_train = data['output_train_model1']\n",
    "model2_train = data['output_train_model2']\n",
    "model3_train = data['output_train_model3']\n",
    "model4_train = data['output_train_model4']\n",
    "model5_train = data['output_train_model5']\n",
    "model6_train = data['output_train_model6']\n",
    "model7_train = data['output_train_model7']\n",
    "model8_train = data['output_train_model8']\n",
    "model9_train = data['output_train_model9']\n",
    "model10_train = data['output_train_model10']\n",
    "model11_train = data['output_train_model11']\n",
    "model12_train = data['output_train_model12']\n",
    "model13_train = data['output_train_model13']\n",
    "model14_train = data['output_train_model14']\n",
    "model1_val = data['output_valid_model1']\n",
    "model2_val = data['output_valid_model2']\n",
    "model3_val = data['output_valid_model3']\n",
    "model4_val = data['output_valid_model4']\n",
    "model5_val = data['output_valid_model5']\n",
    "model6_val = data['output_valid_model6']\n",
    "model7_val = data['output_valid_model7']\n",
    "model8_val = data['output_valid_model8']\n",
    "model9_val = data['output_valid_model9']\n",
    "model10_val = data['output_valid_model10']\n",
    "model11_val = data['output_valid_model11']\n",
    "model12_val = data['output_valid_model12']\n",
    "model13_val = data['output_valid_model13']\n",
    "model14_val = data['output_valid_model14']\n",
    "model1_eval = data['output_test_model1']\n",
    "model2_eval = data['output_test_model2']\n",
    "model3_eval = data['output_test_model3']\n",
    "model4_eval = data['output_test_model4']\n",
    "model5_eval = data['output_test_model5']\n",
    "model6_eval = data['output_test_model6']\n",
    "model7_eval = data['output_test_model7']\n",
    "model8_eval = data['output_test_model8']\n",
    "model9_eval = data['output_test_model9']\n",
    "model10_eval = data['output_test_model10']\n",
    "model11_eval = data['output_test_model11']\n",
    "model12_eval = data['output_test_model12']\n",
    "model13_eval = data['output_test_model13']\n",
    "model14_eval = data['output_test_model14']\n",
    "train_label = data['label_train']\n",
    "valid_label = data['label_valid']\n",
    "eval_label = data['label_test']\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "data_test = np.load(path_test_output_data)\n",
    "model1_test = data_test['output_test_model1']\n",
    "model2_test = data_test['output_test_model2']\n",
    "model3_test = data_test['output_test_model3']\n",
    "model4_test = data_test['output_test_model4']\n",
    "model5_test = data_test['output_test_model5']\n",
    "model6_test = data_test['output_test_model6']\n",
    "model7_test = data_test['output_test_model7']\n",
    "model8_test = data_test['output_test_model8']\n",
    "model9_test = data_test['output_test_model9']\n",
    "model10_test = data_test['output_test_model10']\n",
    "model11_test = data_test['output_test_model11']\n",
    "model12_test = data_test['output_test_model12']\n",
    "model13_test = data_test['output_test_model13']\n",
    "model14_test = data_test['output_test_model14']\n",
    "test_filenames = data_test['filename_test']\n",
    "print('TEST Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_train_prob = softmax(model1_train)\n",
    "model2_train_prob = softmax(model2_train)\n",
    "model3_train_prob = softmax(model3_train)\n",
    "model4_train_prob = softmax(model4_train)\n",
    "model5_train_prob = softmax(model5_train)\n",
    "model6_train_prob = softmax(model6_train)\n",
    "model7_train_prob = softmax(model7_train)\n",
    "model8_train_prob = softmax(model8_train)\n",
    "model9_train_prob = softmax(model9_train)\n",
    "model10_train_prob = softmax(model10_train)\n",
    "model11_train_prob = softmax(model11_train)\n",
    "model12_train_prob = softmax(model12_train)\n",
    "model13_train_prob = softmax(model13_train)\n",
    "model14_train_prob = softmax(model14_train)\n",
    "model1_val_prob = softmax(model1_val) \n",
    "model2_val_prob = softmax(model2_val) \n",
    "model3_val_prob = softmax(model3_val) \n",
    "model4_val_prob = softmax(model4_val) \n",
    "model5_val_prob = softmax(model5_val) \n",
    "model6_val_prob = softmax(model6_val) \n",
    "model7_val_prob = softmax(model7_val) \n",
    "model8_val_prob = softmax(model8_val) \n",
    "model9_val_prob = softmax(model9_val) \n",
    "model10_val_prob = softmax(model10_val) \n",
    "model11_val_prob = softmax(model11_val) \n",
    "model12_val_prob = softmax(model12_val) \n",
    "model13_val_prob = softmax(model13_val) \n",
    "model14_val_prob = softmax(model14_val) \n",
    "model1_eval_prob = softmax(model1_eval)\n",
    "model2_eval_prob = softmax(model2_eval)\n",
    "model3_eval_prob = softmax(model3_eval)\n",
    "model4_eval_prob = softmax(model4_eval)\n",
    "model5_eval_prob = softmax(model5_eval)\n",
    "model6_eval_prob = softmax(model6_eval)\n",
    "model7_eval_prob = softmax(model7_eval)\n",
    "model8_eval_prob = softmax(model8_eval)\n",
    "model9_eval_prob = softmax(model9_eval)\n",
    "model10_eval_prob = softmax(model10_eval)\n",
    "model11_eval_prob = softmax(model11_eval)\n",
    "model12_eval_prob = softmax(model12_eval)\n",
    "model13_eval_prob = softmax(model13_eval)\n",
    "model14_eval_prob = softmax(model14_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate probabilistic outputs of each model\n",
    "\n",
    "train_data_prob = np.concatenate((model1_train_prob, model2_train_prob, model3_train_prob, model4_train_prob, \n",
    "                             model5_train_prob, model6_train_prob, model7_train_prob, model8_train_prob,\n",
    "                             model9_train_prob, model10_train_prob, model11_train_prob, model12_train_prob, \n",
    "                             model13_train_prob, model14_train_prob), axis = 1)\n",
    "\n",
    "valid_data_prob = np.concatenate((model1_val_prob, model2_val_prob, model3_val_prob, model4_val_prob, model5_val_prob,\n",
    "                            model6_val_prob, model7_val_prob, model8_val_prob, model9_val_prob, model10_val_prob, \n",
    "                             model11_val_prob, model12_val_prob, model13_val_prob, model14_val_prob), axis = 1)\n",
    "\n",
    "eval_data_prob = np.concatenate((model1_eval_prob, model2_eval_prob, model3_eval_prob, model4_eval_prob, model5_eval_prob,\n",
    "                            model6_eval_prob, model7_eval_prob, model8_eval_prob, model9_eval_prob, model10_eval_prob, \n",
    "                           model11_eval_prob, model12_eval_prob, model13_eval_prob, model14_eval_prob), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate outputs of each model\n",
    "\n",
    "train_data = np.concatenate((model1_train, model2_train, model3_train, model4_train, \n",
    "                             model5_train, model6_train, model7_train, model8_train, \n",
    "                             model9_train, model10_train, model11_train, model12_train, \n",
    "                             model13_train, model14_train), axis = 1)\n",
    "\n",
    "valid_data = np.concatenate((model1_val, model2_val, model3_val, model4_val, model5_val,\n",
    "                            model6_val, model7_val, model8_val, model9_val, \n",
    "                             model10_val, model11_val, model12_val, model13_val, model14_val), axis = 1)\n",
    "\n",
    "eval_data = np.concatenate((model1_eval, model2_eval, model3_eval, model4_eval, model5_eval,\n",
    "                            model6_eval, model7_eval, model8_eval, model9_eval, model10_eval, model11_eval, \n",
    "                            model12_eval, model13_eval, model14_eval), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'linear', C = 0.00009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(train_data_prob, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.3576672104\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = clf.score(train_data_prob, train_label)\n",
    "print(train_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.4019754565\n"
     ]
    }
   ],
   "source": [
    "valid_accuracy = clf.score(valid_data_prob, valid_label)\n",
    "print(valid_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.5\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = clf.score(eval_data_prob, eval_label)\n",
    "print(eval_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test Set Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test_prob = softmax(model1_test)\n",
    "model2_test_prob = softmax(model2_test)\n",
    "model3_test_prob = softmax(model3_test)\n",
    "model4_test_prob = softmax(model4_test)\n",
    "model5_test_prob = softmax(model5_test)\n",
    "model6_test_prob = softmax(model6_test)\n",
    "model7_test_prob = softmax(model7_test)\n",
    "model8_test_prob = softmax(model8_test)\n",
    "model9_test_prob = softmax(model9_test)\n",
    "model10_test_prob = softmax(model10_test)\n",
    "model11_test_prob = softmax(model11_test)\n",
    "model12_test_prob = softmax(model12_test)\n",
    "model13_test_prob = softmax(model13_test)\n",
    "model14_test_prob = softmax(model14_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate probabilistic outputs of each model\n",
    "\n",
    "test_data_prob = np.concatenate((model1_test_prob, model2_test_prob, model3_test_prob,\n",
    "                                model4_test_prob, model5_test_prob, model6_test_prob, \n",
    "                                model7_test_prob, model8_test_prob, model9_test_prob, \n",
    "                                model10_test_prob, model11_test_prob, model12_test_prob, \n",
    "                                model13_test_prob, model14_test_prob), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate outputs of each model\n",
    "\n",
    "test_data = np.concatenate((model1_test, model2_test, model3_test, model4_test, model5_test, model6_test, \n",
    "                            model7_test, model8_test, model9_test, model10_test, model11_test, model12_test, \n",
    "                            model13_test, model14_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = clf.predict(test_data_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_filenames)):\n",
    "    with open(submission_folder + test_filenames[i] + '.txt', 'w+') as f:\n",
    "        f.write(label_to_name[predictions_test[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
